{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong Baseline: Fine-tuned mBART (Complete)\n",
    "\n",
    "**This notebook does EVERYTHING:**\n",
    "1. ‚úÖ Trains mBART on CS-Sum dataset\n",
    "2. ‚úÖ Generates predictions on test set\n",
    "3. ‚úÖ Evaluates with ROUGE, BERTScore, CMC\n",
    "4. ‚úÖ Shows results\n",
    "5. ‚úÖ Downloads predictions + scores\n",
    "\n",
    "**Run on Google Colab with GPU enabled!**\n",
    "\n",
    "**Time:** 2-4 hours (mostly training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies\n",
    "!pip install -q transformers==4.36.0 datasets==2.16.0 accelerate==0.25.0\n",
    "!pip install -q sentencepiece==0.1.99 rouge-score==0.1.2 bert-score==0.3.13\n",
    "!pip install -q langdetect==1.0.9\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score_fn\n",
    "from langdetect import detect_langs, LangDetectException\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be VERY slow.\")\n",
    "    print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Your Data\n",
    "\n",
    "**Click the folder icon on the left ‚Üí** Upload these files:\n",
    "- `cs_sum_train.jsonl`\n",
    "- `cs_sum_dev.jsonl`\n",
    "- `cs_sum_test.jsonl`\n",
    "\n",
    "Then run the cell below to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if files exist\n",
    "required_files = ['cs_sum_train.jsonl', 'cs_sum_dev.jsonl', 'cs_sum_test.jsonl']\n",
    "all_present = True\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        lines = sum(1 for _ in open(file))\n",
    "        print(f\"‚úÖ {file}: {lines} examples\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file}: NOT FOUND\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n‚úÖ All data files present! Ready to proceed.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Please upload the missing files before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Convert to format for training.\"\"\"\n",
    "    prepared = []\n",
    "    for item in data:\n",
    "        # Concatenate thread messages\n",
    "        messages = item.get('messages', [])\n",
    "        thread_text = ' '.join([msg['text'] for msg in messages])\n",
    "        \n",
    "        # Get summary\n",
    "        summary = item.get('summary', '')\n",
    "        \n",
    "        if thread_text and summary:\n",
    "            prepared.append({\n",
    "                'thread_id': item.get('thread_id', ''),\n",
    "                'thread': thread_text,\n",
    "                'summary': summary\n",
    "            })\n",
    "    \n",
    "    return prepared\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_data = prepare_data(load_jsonl('cs_sum_train.jsonl'))\n",
    "dev_data = prepare_data(load_jsonl('cs_sum_dev.jsonl'))\n",
    "test_data = prepare_data(load_jsonl('cs_sum_test.jsonl'))\n",
    "\n",
    "print(f\"Train: {len(train_data)} examples\")\n",
    "print(f\"Dev: {len(dev_data)} examples\")\n",
    "print(f\"Test: {len(test_data)} examples\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample:\")\n",
    "print(f\"Thread: {train_data[0]['thread'][:150]}...\")\n",
    "print(f\"Summary: {train_data[0]['summary']}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "dev_dataset = Dataset.from_list(dev_data)\n",
    "test_dataset = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.num_parameters():,} parameters\")\n",
    "print(f\"‚úÖ Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization parameters\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# Set source and target languages\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"en_XX\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize inputs and targets.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        examples['thread'],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(\n",
    "            examples['summary'],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "    \n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dev = dev_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "print(\"‚úÖ Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_checkpoints\",\n",
    "    \n",
    "    # Training config\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Generation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    \n",
    "    # Misc\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# ROUGE scorer for training evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ROUGE during training.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        scores = scorer.score(label, pred)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge_scores['rouge1']),\n",
    "        'rouge2': np.mean(rouge_scores['rouge2']),\n",
    "        'rougeL': np.mean(rouge_scores['rougeL'])\n",
    "    }\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: TRAIN! üöÄ\n",
    "\n",
    "**This will take 2-4 hours. Go get coffee! ‚òï**\n",
    "\n",
    "Monitor:\n",
    "- Loss should decrease\n",
    "- ROUGE scores should increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take ~2-4 hours. Monitor the progress below.\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating predictions on test set...\")\n",
    "print(f\"Processing {len(test_data)} examples...\\n\")\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "# Generate predictions\n",
    "for item in tqdm(test_data):\n",
    "    inputs = tokenizer(\n",
    "        item['thread'],\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    predictions_list.append({\n",
    "        'thread_id': item['thread_id'],\n",
    "        'prediction': summary,\n",
    "        'reference': item['summary'],\n",
    "        'thread': item['thread']\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(predictions_list)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate with All Metrics\n",
    "\n",
    "Now we compute:\n",
    "1. ROUGE (content selection)\n",
    "2. BERTScore (semantic similarity)\n",
    "3. CMC (code-mixing coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and references\n",
    "predictions = [p['prediction'] for p in predictions_list]\n",
    "references = [p['reference'] for p in predictions_list]\n",
    "threads = [p['thread'] for p in predictions_list]\n",
    "\n",
    "print(\"Computing evaluation metrics...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ROUGE Scores\n",
    "print(\"üìä Computing ROUGE...\")\n",
    "\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "for pred, ref in zip(predictions, references):\n",
    "    if pred and ref:\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "rouge_results = {\n",
    "    'rouge1_f1': np.mean(rouge_scores['rouge1']),\n",
    "    'rouge2_f1': np.mean(rouge_scores['rouge2']),\n",
    "    'rougeL_f1': np.mean(rouge_scores['rougeL'])\n",
    "}\n",
    "\n",
    "print(\"‚úÖ ROUGE computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BERTScore\n",
    "print(\"üß† Computing BERTScore (this takes ~1 minute)...\")\n",
    "\n",
    "P, R, F1 = bert_score_fn(\n",
    "    predictions,\n",
    "    references,\n",
    "    lang='en',\n",
    "    model_type='bert-base-multilingual-cased',\n",
    "    verbose=False,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "bertscore_results = {\n",
    "    'bertscore_precision': P.mean().item(),\n",
    "    'bertscore_recall': R.mean().item(),\n",
    "    'bertscore_f1': F1.mean().item()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ BERTScore computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Code-Mixing Coverage (CMC)\n",
    "print(\"üåç Computing Code-Mixing Coverage...\")\n",
    "\n",
    "def detect_language_distribution(text):\n",
    "    \"\"\"Detect language distribution.\"\"\"\n",
    "    if not text:\n",
    "        return {}\n",
    "    \n",
    "    words = text.split()\n",
    "    lang_counts = Counter()\n",
    "    total = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            langs = detect_langs(word)\n",
    "            if langs:\n",
    "                lang_counts[langs[0].lang] += 1\n",
    "                total += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if total == 0:\n",
    "        return {}\n",
    "    \n",
    "    return {lang: count / total for lang, count in lang_counts.items()}\n",
    "\n",
    "cmc_scores = []\n",
    "\n",
    "for pred, thread in zip(predictions, threads):\n",
    "    if not pred or not thread:\n",
    "        cmc_scores.append(0.5)\n",
    "        continue\n",
    "    \n",
    "    thread_langs = detect_language_distribution(thread)\n",
    "    pred_langs = detect_language_distribution(pred)\n",
    "    \n",
    "    if not thread_langs or not pred_langs:\n",
    "        cmc_scores.append(0.5)\n",
    "        continue\n",
    "    \n",
    "    all_langs = set(list(thread_langs.keys()) + list(pred_langs.keys()))\n",
    "    ratio_diff = sum(abs(thread_langs.get(l, 0) - pred_langs.get(l, 0)) for l in all_langs)\n",
    "    cmc = max(0.0, 1.0 - (ratio_diff / 2.0))\n",
    "    cmc_scores.append(cmc)\n",
    "\n",
    "cmc_results = {\n",
    "    'code_mixing_coverage': np.mean(cmc_scores)\n",
    "}\n",
    "\n",
    "print(\"‚úÖ CMC computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all scores\n",
    "all_scores = {**rouge_results, **bertscore_results, **cmc_results}\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS - mBART Fine-tuned\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Content Selection (ROUGE):\")\n",
    "print(f\"  ROUGE-1 F1      : {all_scores['rouge1_f1']:.4f}\")\n",
    "print(f\"  ROUGE-2 F1      : {all_scores['rouge2_f1']:.4f}\")\n",
    "print(f\"  ROUGE-L F1      : {all_scores['rougeL_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nüß† Semantic Similarity (BERTScore):\")\n",
    "print(f\"  Precision       : {all_scores['bertscore_precision']:.4f}\")\n",
    "print(f\"  Recall          : {all_scores['bertscore_recall']:.4f}\")\n",
    "print(f\"  F1              : {all_scores['bertscore_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nüåç Bilingual Faithfulness (Novel Metric):\")\n",
    "print(f\"  CMC             : {all_scores['code_mixing_coverage']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "if all_scores['rougeL_f1'] > 0.30:\n",
    "    print(\"  ‚úÖ ROUGE-L > 0.30: Good content selection\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è ROUGE-L < 0.30: Below target\")\n",
    "\n",
    "if all_scores['bertscore_f1'] > 0.70:\n",
    "    print(\"  ‚úÖ BERTScore > 0.70: Strong semantic match\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è BERTScore < 0.70: Below target\")\n",
    "\n",
    "if all_scores['code_mixing_coverage'] > 0.70:\n",
    "    print(\"  ‚úÖ CMC > 0.70: Good language preservation\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è CMC < 0.70: Language collapse detected\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Show Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(min(3, len(predictions_list))):\n",
    "    example = predictions_list[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Thread (first 150 chars): {example['thread'][:150]}...\")\n",
    "    print(f\"\\nPrediction: {example['prediction']}\")\n",
    "    print(f\"\\nReference:  {example['reference']}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Results for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "with open('mbart_predictions.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in predictions_list:\n",
    "        f.write(json.dumps({\n",
    "            'thread_id': item['thread_id'],\n",
    "            'prediction': item['prediction']\n",
    "        }, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"‚úÖ Saved: mbart_predictions.jsonl\")\n",
    "\n",
    "# Save scores\n",
    "with open('mbart_scores.json', 'w') as f:\n",
    "    json.dump(all_scores, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved: mbart_scores.json\")\n",
    "\n",
    "# Save detailed results with references\n",
    "with open('mbart_detailed_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(predictions_list, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Saved: mbart_detailed_results.json\")\n",
    "\n",
    "print(\"\\nüì• Download these files:\")\n",
    "print(\"  1. mbart_predictions.jsonl (for submission)\")\n",
    "print(\"  2. mbart_scores.json (metrics)\")\n",
    "print(\"  3. mbart_detailed_results.json (full results)\")\n",
    "print(\"\\nClick the folder icon ‚Üí Right-click each file ‚Üí Download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Create Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_report = f\"\"\"\n",
    "{'='*60}\n",
    "MILESTONE 2 - STRONG BASELINE SUMMARY\n",
    "{'='*60}\n",
    "\n",
    "Model: mBART-large-50 (611M parameters)\n",
    "Dataset: CS-Sum (Chinese-English code-mixed)\n",
    "Training: {len(train_data)} examples, 3 epochs\n",
    "Test Set: {len(test_data)} examples\n",
    "\n",
    "RESULTS:\n",
    "{'-'*60}\n",
    "ROUGE-1 F1      : {all_scores['rouge1_f1']:.4f}\n",
    "ROUGE-2 F1      : {all_scores['rouge2_f1']:.4f}\n",
    "ROUGE-L F1      : {all_scores['rougeL_f1']:.4f}\n",
    "\n",
    "BERTScore P     : {all_scores['bertscore_precision']:.4f}\n",
    "BERTScore R     : {all_scores['bertscore_recall']:.4f}\n",
    "BERTScore F1    : {all_scores['bertscore_f1']:.4f}\n",
    "\n",
    "CMC             : {all_scores['code_mixing_coverage']:.4f}\n",
    "{'-'*60}\n",
    "\n",
    "KEY FINDING:\n",
    "Fine-tuned mBART achieves ROUGE-L = {all_scores['rougeL_f1']:.2f}\n",
    "but shows {'language collapse' if all_scores['code_mixing_coverage'] < 0.70 else 'good language preservation'}\n",
    "(CMC = {all_scores['code_mixing_coverage']:.2f})\n",
    "\n",
    "Training completed in: {train_result.metrics['train_runtime']:.0f} seconds\n",
    "                       ({train_result.metrics['train_runtime']/3600:.1f} hours)\n",
    "\n",
    "{'='*60}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save report\n",
    "with open('training_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n‚úÖ Saved: training_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ DONE!\n",
    "\n",
    "**You now have:**\n",
    "1. ‚úÖ Trained mBART model\n",
    "2. ‚úÖ Test predictions (`mbart_predictions.jsonl`)\n",
    "3. ‚úÖ Evaluation scores (`mbart_scores.json`)\n",
    "4. ‚úÖ Complete results (`mbart_detailed_results.json`)\n",
    "5. ‚úÖ Summary report (`training_summary.txt`)\n",
    "\n",
    "**Next steps:**\n",
    "1. Download all the files (click folder icon ‚Üí right-click ‚Üí download)\n",
    "2. Use these results in your `strong-baseline.md`\n",
    "3. Include in your final report\n",
    "\n",
    "**For your report, you can say:**\n",
    "> \"We fine-tuned mBART-large-50 on 2,584 Chinese-English code-mixed conversations for 3 epochs (~{train_result.metrics['train_runtime']/3600:.1f} hours on T4 GPU). On the test set of 325 examples, the model achieved ROUGE-L = {all_scores['rougeL_f1']:.3f}, BERTScore F1 = {all_scores['bertscore_f1']:.3f}, and CMC = {all_scores['code_mixing_coverage']:.3f}.\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
